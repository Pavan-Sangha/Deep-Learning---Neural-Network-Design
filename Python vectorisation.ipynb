{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorisation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can obtain $z = w^T*x+b$ by using np.dot in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5236\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3,50,100])\n",
    "y = np.array([3,4,5,10,47])\n",
    "b = 10\n",
    "z = np.dot(x,y)+b\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed up in time for vectorisation\n",
    "* vectorisation exploits parallelisation in both GPU's and CPU's and as a result are a lot faster than for loops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorised version in milliseconds :9.408950805664062ms\n",
      "looped version in milliseconds :426.32484436035156ms\n",
      "The speed up by using vectorisation is 45.31056152442732 times\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "# initialise two random vectors of dimension 1 million\n",
    "\n",
    "a = np.random.rand(10000000)\n",
    "b = np.random.rand(10000000)\n",
    "\n",
    "# We start with the vectorised version\n",
    "tic = time.time()\n",
    "\n",
    "c = np.dot(a,b)\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "diff_vectorised = toc - tic\n",
    "\n",
    "print('Vectorised version in milliseconds :'+str(diff_vectorised*1000)+'ms')\n",
    "\n",
    "# Next we try the for loop\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "sum = 0\n",
    "for index in range(1000000):\n",
    "    sum += a[index]*b[index]\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "diff_looped = toc - tic\n",
    "\n",
    "print('looped version in milliseconds :'+str(diff_looped*1000)+'ms')\n",
    "\n",
    "print('The speed up by using vectorisation is '+str(1/(diff_vectorised/diff_looped))+ ' times')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.48797317e+01 2.68811714e+43 7.38905610e+00]\n"
     ]
    }
   ],
   "source": [
    "v = np.array([2.7,100,2])\n",
    "b = np.exp(v) # elementwise exponential operation\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorising logistic regression forward pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recall we have $m$ training examples $(x_{1},y_{1}),\\ldots,(x_{m},y_{m})$ with $x_{i} \\in \\mathbb{R}^{n_{x}}$\n",
    "* For each example we compute $z_{i} = w^{T}x_{i}+b$ and then $a_{i} = \\sigma(z_{i})$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "\n",
    "n_x = 10    # feature dimensionality\n",
    "m = 3 # training sample size \n",
    "x1 = np.random.rand(n_x)\n",
    "x2 = np.random.rand(n_x)\n",
    "x3 = np.random.rand(n_x)\n",
    "y1 = 0\n",
    "y2 = 1\n",
    "y3 = 0\n",
    "\n",
    "y = np.array([y1,y2,y3])\n",
    "w = np.array([100,2,3,5,10,200,30,1,6,8])   # weights vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39732429 0.0879649  0.8744926 ]\n",
      " [0.8491405  0.88313677 0.6331414 ]\n",
      " [0.55149249 0.72980439 0.96729557]\n",
      " [0.05238606 0.97006001 0.00266972]\n",
      " [0.34761441 0.69797405 0.64967418]\n",
      " [0.58261928 0.22312668 0.95458838]\n",
      " [0.52374288 0.38721469 0.82597694]\n",
      " [0.93689477 0.07932701 0.41658208]\n",
      " [0.02695664 0.54463197 0.0602862 ]\n",
      " [0.16167373 0.43218378 0.66454221]]\n"
     ]
    }
   ],
   "source": [
    "X = np.stack((x1,x2,x3),axis = -1)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([191.45142765,  97.62858254, 329.91914098])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.dot(w.T,X)+10\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Equivalent to A = [z1,z2,z3]\n",
    "# Since Z values are large a values will be near 1\n",
    "A = sigmoid(z)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorising linear regression backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Loss Function\n",
    "* Recall we have $m$ training examples $(x_{1},y_{1}),\\ldots,(x_{m},y_{m})$ with $x_{i} \\in \\mathbb{R}^{n_{x}}$\n",
    "* For each example we compute $z_{i} = w^{T}x_{i}+b$ and then $a_{i} = \\sigma(z_{i})$\n",
    "\n",
    "* The loss function w.r.t one example say example $i$ is $L(a_{i},y_{i})=-y_{i}\\mathrm{log}(a_{i})+(1-y_{i})\\mathrm{log}(1-a_{i})$ where $a_{i} = \\sigma(z_{i})$\n",
    "* 'dai' is then $\\frac{\\partial L}{\\partial a_{i}} = \\frac{-y}{a_{i}}+\\frac{1-y}{1-a_{i}}$\n",
    "* 'dzi' is then $\\frac{\\partial L}{\\partial z_{i}} = \\frac{\\partial L}{\\partial a_{i}} \\cdot \\frac{\\partial a_{i}}{\\partial z_{i}} = a_{i}-y$\n",
    "* 'dwi' is then $\\frac{\\partial L}{\\partial w_{i}} = x_{i}dz_{i}$\n",
    "* 'dbi' is then $\\frac{\\partial L}{\\partial b_{i}} = dz_{i}$\n",
    "\n",
    "\n",
    "\n",
    "Cost Function\n",
    "* Now that we have a value for the loss w.r.t a single training example $1 \\leq i \\leq m$. We average to get the cost function $$J(w,b) = \\frac{1}{m} \\sum_{i=1}^{m} L(a^{i},y)$$\n",
    "* We can distribute the partial derivatives across the sum for any variable we are interested in as follows\n",
    "$$ \\frac{\\partial J}{\\partial w_{i}} = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial}{\\partial w_{i}} L(a_{i},y^{i}) $$\n",
    "* In our example dz = [dz1,dz2,dz3] = [a1-y1,a2-y2,a3-y3] = A-y\n",
    "* $db = \\frac{1}{m} \\sum_{i=1}^{m} dz_{i}$ and we can calculate this as db = 1/m * np.sum(dz) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
